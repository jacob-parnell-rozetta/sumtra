# Script to cascade the joint MT-LM and MS-LM models
import os
from typing import Optional, Tuple
import torch
from torch import nn, Tensor
from transformers import AutoModelForSeq2SeqLM


class CascadeModelForXLS(nn.Module):
    def __init__(self, tokenizer, model_mt, model_ms, max_input_len, max_output_len, freeze_strategy,
                 t3l_arguments):
        super().__init__()
        self.embed_dim = 512  # config.d_model
        self.freeze_strategy = freeze_strategy
        self.tokenizer = tokenizer
        # Load MT-LM (machine translation language model) - Tra
        self.mtlm = AutoModelForSeq2SeqLM.from_pretrained(model_mt)
        # Load MS-LM (monolingual summarization language model) - Sum
        self.mslm = AutoModelForSeq2SeqLM.from_pretrained(model_ms)
        self.freeze_params()
        self.mtlm_emb_matrix = self.mtlm.get_input_embeddings().weight  # for embedding mixture use downstream LM
        # Max lengths
        self.max_input_len = max_input_len
        self.max_output_len = max_output_len
        # softmax temp
        self.temperature = t3l_arguments['tau']  # default = 1.0
        # use gumbel softmax
        self.gumbel_softmax = t3l_arguments['gumbel_softmax']  # default = False
        # use hard predictions
        self.use_hard = t3l_arguments['use_hard_predictions']  # default = False
        # use auxiliary loss - reinforcement learning, token alignment, teacher forcing
        self.auxiliary_loss = t3l_arguments['auxiliary_loss']  # default None

    def forward(
        self,
        query,
        attention_mask: Optional[Tensor] = None,
        decoder_input_ids: Optional[Tensor] = None,
        decoder_attention_mask: Optional[Tensor] = None,
        token_type_ids: Optional[Tensor] = None,
        need_weights=False,
        output_attentions=False,
        test_trans=None,
        backtranslation_ids: Optional[Tensor] = None,
        scorer: Optional[Tuple] = None
    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]:

        bsz, src_len = query.size()
        # assert embed_dim == self.embed_dim
        assert list(query.size()) == [bsz, src_len]
        assert attention_mask is not None

        # Actual batch size
        actual_bsz = int(bsz / 2)
        # print(f"**** DEBUG: {query}\n")
        # Seq2seq model that outputs the translation probabilities for the tokens
        """https://huggingface.co/docs/transformers/model_doc/mbart#training-of-mbart50"""
        summarization_preds = self.mslm.generate_t3l(input_ids=query, attention_mask=attention_mask, output_scores=True,
                                                     return_dict_in_generate=True, num_beams=1, num_beam_groups=1,
                                                     do_sample=False,
                                                     output_hidden_states=True, output_attentions=True, # for BT tests
                                                     decoder_start_token_id=self.tokenizer.eos_token_id,
                                                     forced_bos_token_id=self.tokenizer.lang_code_to_id[self.tokenizer.src_lang],  # want en -> en
                                                     max_length=self.max_output_len)  # should be summary output len

        # Extract the probability distributions generated by the summarizer
        probs = summarization_preds.scores  # .mean(dim=2)
        # Extract the corresponding sentences
        sum_ids_bos = summarization_preds.sequences
        sum_ids = sum_ids_bos[:, 1:]  # removes BOS token to align with labels

        if self.auxiliary_loss == 'teacher_forcing':
            # forward pass for backtranslation input
            backtranslation_input_ids = backtranslation_ids[:, :-1]
            bt_attention_mask = (backtranslation_input_ids != self.tokenizer.pad_token_id)
            decoder_outputs = self.mslm.model.decoder(input_ids=backtranslation_input_ids,
                                                      attention_mask=bt_attention_mask,
                                                      encoder_hidden_states=summarization_preds.encoder_hidden_states[-1],
                                                      encoder_attention_mask=attention_mask,
                                                      output_attentions=True,
                                                      return_dict=True)
            decoder_hidden_states = decoder_outputs.last_hidden_state
            # Same as modeling_mbart.py
            backtranslation_logits = self.mslm.get_output_embeddings()(decoder_hidden_states) + self.mslm.final_logits_bias
        else:
            backtranslation_logits = None

        # if self.use_hard:
        #     # Create attention mask for MT-LM
        #     att_bsz, att_seq_len = sum_ids_bos.size()
        #     lm_attention_mask = torch.ones((att_bsz, att_seq_len), device=sum_ids_bos.device)  # .cuda()
        #     lm_attention_mask.masked_fill_(sum_ids_bos == self.tokenizer.pad_token_id, 0)
        #     # Create decoder embeddings for MT-LM decoder to allow loss calculation
        #     dec_att_bsz, dec_att_seq_len = decoder_input_ids.size()
        #     dec_lm_attention_mask = torch.ones((dec_att_bsz, dec_att_seq_len),
        #                                        device=decoder_input_ids.device)  # .cuda()
        #     dec_lm_attention_mask.masked_fill_(decoder_input_ids == self.tokenizer.pad_token_id, 0)
        #
        #     outputs = self.mtlm(input_ids=sum_ids_bos, attention_mask=lm_attention_mask,
        #                         decoder_input_ids=decoder_input_ids, decoder_attention_mask=dec_lm_attention_mask,
        #                         bos_token_id=self.tokenizer.lang_code_to_id[self.tokenizer.tgt_lang],
        #                         )

        # else:  # use soft embeddings to couple the models
        # Compute average embeddings with probability distribution
        e_embs = self.expected_embeddings(probs)

        # Final LM for translation -> bos token for translator should be target language ID
        bos_token_embed = self.mtlm_emb_matrix[self.tokenizer.lang_code_to_id[self.tokenizer.tgt_lang], :]
        pad_token_embed = self.mtlm_emb_matrix[self.tokenizer.pad_token_id]

        # Create attention mask for MT-LM
        att_bsz, att_seq_len, _ = e_embs.size()
        lm_attention_mask = torch.ones((att_bsz, att_seq_len), device=e_embs.device)  # .cuda()
        lm_attention_mask.masked_fill_(sum_ids == self.tokenizer.pad_token_id, 0)

        # Create decoder embeddings for MT-LM decoder to allow loss calculation
        decoder_input_embeds = self.mtlm.model.encoder.embed_tokens(decoder_input_ids) * self.mtlm.model.encoder.embed_scale
        dec_att_bsz, dec_att_seq_len, _ = decoder_input_embeds.size()
        dec_lm_attention_mask = torch.ones((dec_att_bsz, dec_att_seq_len), device=decoder_input_embeds.device)  # .cuda()
        dec_lm_attention_mask.masked_fill_(decoder_input_ids == self.tokenizer.pad_token_id, 0)

        outputs = self.mtlm(inputs_embeds=e_embs, attention_mask=lm_attention_mask,
                            decoder_inputs_embeds=decoder_input_embeds,
                            decoder_attention_mask=dec_lm_attention_mask,
                            pad_token_embed=pad_token_embed,
                            bos_token_id=self.tokenizer.lang_code_to_id[self.tokenizer.tgt_lang],
                            bos_token_embed=bos_token_embed,)

        return outputs.logits, backtranslation_logits, sum_ids_bos, e_embs

    def expected_embeddings(self, probabilities):
        """
        Calculate the expected input embeddings for the LM model with the generated probability distributions.
        :param probabilities: The probability distributions generated by the Sum model.
        :return: Expected embeddings (torch.tensor)
        """

        # Obtain sizes
        batch_size, max_seq_len, vocab_size = probabilities.size()
        if self.gumbel_softmax:
            probabilities = torch.nn.functional.gumbel_softmax(probabilities, tau=self.temperature, hard=False, dim=-1)
        else:
            probabilities = torch.softmax(probabilities / self.temperature, dim=-1)

        # Obtain embeding_dimensions of downstream LM
        emb_vocab_size, emb_dim = self.mtlm_emb_matrix.size()

        # Calculate the expectation
        probabilities = probabilities.view(-1, vocab_size)
        expected_embs = probabilities.mm(self.mtlm_emb_matrix)
        expected_embs = expected_embs.view(-1, max_seq_len, emb_dim)

        return expected_embs

    def freeze_params(self):

        if self.freeze_strategy == 'train_1-2':
            print('Fixed Tra! Training Sum')
            for param in self.mtlm.parameters():
                param.requires_grad = False
        elif self.freeze_strategy == 'train_3-4':
            print('Fixed Sum! Training Tra')
            for param in self.mslm.parameters():
                param.requires_grad = False
        elif self.freeze_strategy == 'train_2-3':
            print('Training coupling!')
            for name, param in self.mslm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
            for name, param in self.mtlm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_all':
            print('Train all!')

        # Added tests
        elif self.freeze_strategy == 'train_2':
            print('Training Sum Decoder only')
            for name, param in self.mslm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
            for name, param in self.mtlm.named_parameters():
                if 'decoder' in name or 'encoder' in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_1':
            print('Training Sum Encoder only')
            for name, param in self.mslm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
            for name, param in self.mtlm.named_parameters():
                if 'decoder' in name or 'encoder' in name:
                    param.requires_grad = False

        elif self.freeze_strategy == 'train_4':
            print('Training Tra Decoder only')
            for name, param in self.mtlm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
            for name, param in self.mslm.named_parameters():
                if 'decoder' in name or 'encoder' in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_3':
            print('Training Tra Encoder only')
            for name, param in self.mtlm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
            for name, param in self.mslm.named_parameters():
                if 'decoder' in name or 'encoder' in name:
                    param.requires_grad = False

        # Train alternate modules
        elif self.freeze_strategy == 'train_2-4':
            print('Training Sum and Tra decoders only')
            for name, param in self.mslm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
            for name, param in self.mtlm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_1-3':
            print('Training Sum and Tra encoders only')
            for name, param in self.mslm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
            for name, param in self.mtlm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_1-4':
            print('Training Sum encoder and Tra decoder only')
            for name, param in self.mslm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
            for name, param in self.mtlm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False

        # Train 3 modules
        elif self.freeze_strategy == 'train_2-3-4':
            print('Freezing Sum encoder only')
            for name, param in self.mslm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_1-3-4':
            print('Freezing Sum decoder only')
            for name, param in self.mslm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_1-2-4':
            print('Freezing Tra encoder only')
            for name, param in self.mtlm.named_parameters():
                if 'decoder' not in name:
                    param.requires_grad = False
        elif self.freeze_strategy == 'train_1-2-3':
            print('Freezing Tra decoder only')
            for name, param in self.mtlm.named_parameters():
                if 'encoder' not in name:
                    param.requires_grad = False
